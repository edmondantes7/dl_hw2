{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "import glob\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img_array(path):                                                            \n",
    "    \"\"\"                                                                             \n",
    "    Given path of image, returns it's numpy array                                   \n",
    "    \"\"\"                                                                             \n",
    "    return imageio.imread(path)                                                  \n",
    "                                                                                    \n",
    "def get_files(folder):                                                              \n",
    "    \"\"\"                                                                             \n",
    "    Given path to folder, returns list of files in it                               \n",
    "    \"\"\"                                                                             \n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]                          \n",
    "    filenames.sort()                                                                \n",
    "    return filenames                                                                \n",
    "                                                                                    \n",
    "def get_label(filepath, label2id):                                                  \n",
    "    \"\"\"                                                                             \n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png                  \n",
    "    Returns label for a filepath                                                    \n",
    "    \"\"\"                                                                             \n",
    "    tokens = filepath.split('/')                                                    \n",
    "    label = tokens[-1].split('_')[1][:-4]                                           \n",
    "    if label in label2id:                                                           \n",
    "        return label2id[label]                                                      \n",
    "    else:                                                                           \n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_labels(folder, label2id):                                                \n",
    "    \"\"\"                                                                          \n",
    "    Returns vector of labels extracted from filenames of all files in folder     \n",
    "    :param folder: path to data folder                                           \n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"                                                                          \n",
    "    files = get_files(folder)                                                    \n",
    "    y = []                                                                       \n",
    "    for f in files:                                                              \n",
    "        y.append(get_label(f,label2id))                                          \n",
    "    return np.array(y)                                                                                                                                                                                   \n",
    "                                                                                 \n",
    "def get_label_mapping(label_file):                                               \n",
    "    \"\"\"                                                                          \n",
    "    Returns mappings of label to index and index to label                        \n",
    "    The input file has list of labels, each on a separate line.                  \n",
    "    \"\"\"                                                                          \n",
    "    with open(label_file, 'r') as f:                                             \n",
    "        id2label = f.readlines()                                                 \n",
    "        id2label = [l.strip() for l in id2label]                                 \n",
    "    label2id = {}                                                                \n",
    "    count = 0                                                                    \n",
    "    for label in id2label:                                                       \n",
    "        label2id[label] = count                                                  \n",
    "        count += 1                                                               \n",
    "    return id2label, label2id \n",
    "\n",
    "def get_images(folder):                                                          \n",
    "    \"\"\"                                                                          \n",
    "    returns numpy array of all samples in folder                                 \n",
    "    each column is a sample resized to 30x30 and flattened                       \n",
    "    \"\"\"                                                                          \n",
    "    files = get_files(folder)                                                    \n",
    "    images = []                                                                  \n",
    "    count = 0                                                                    \n",
    "\n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):                                              \n",
    "    \"\"\"                                                                          \n",
    "    Return X and y                                                               \n",
    "    \"\"\"                                                                          \n",
    "    train_data_path = data_root_path + 'train'                                   \n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')          \n",
    "    X = get_images(train_data_path)                                              \n",
    "    y = get_labels(train_data_path, label2id)                                    \n",
    "    return X, y                                                                  \n",
    "                                                                                 \n",
    "def save_predictions(filename, y):                                               \n",
    "    \"\"\"                                                                          \n",
    "    Dumps y into .npy file                                                       \n",
    "    \"\"\"                                                                          \n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "data_root_path = 'cifar10-hw2/'\n",
    "train_x, train_y = get_train_data(data_root_path) # this may take a few minutes\n",
    "test_x = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Use to reset Tensorboard graph visualizations across runs \"\"\"\n",
    "def reset_graph():\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "\"\"\" \n",
    "    - Creates dir \"tf_logs\" in working direction to store Tensorboard events\n",
    "    - Runs will appear w/in tf_logs directory as 'run-{timestamp}' to be used by TB\n",
    "\"\"\"\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_CNN(object):\n",
    "    def __init__(self, X_data, Y_data, percentage_test=.1, random_seed=42):\n",
    "        reset_graph()\n",
    "        \n",
    "        # model from memmory\n",
    "        tf.set_random_seed(42)\n",
    "        np.random.seed(42)\n",
    "        self.perc_test = percentage_test\n",
    "        self.num_examples = X_data.shape[0]\n",
    "        cutoff = int(self.num_examples * self.perc_test)\n",
    "        \n",
    "        indices = np.arange(self.num_examples)\n",
    "        np.random.shuffle(indices)\n",
    "        test_indices = indices[:cutoff]\n",
    "        train_indices = indices[cutoff:]\n",
    "\n",
    "        # select training sets w/ holdout\n",
    "        self.train_x = X_data[train_indices, :, :, :]\n",
    "        self.train_y = Y_data[train_indices]\n",
    "\n",
    "        # select testing sets w/ holdout\n",
    "        self.test_x = train_x[test_indices, :, :, :]\n",
    "        self.test_y = train_y[test_indices]\n",
    "    \n",
    "    def build_graph(self, convolutional_dimensions=[64, 128], dense_layer_dimensions=[1024, 128]):\n",
    "\n",
    "        with tf.name_scope(\"input\"):\n",
    "            last_layer = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"x_input\")\n",
    "            y = tf.placeholder(tf.int64, [None], name=\"y_input\")\n",
    "            global_step = tf.placeholder(tf.int64, [], name=\"global_step\")\n",
    "            \n",
    "        # Normalize input on input data\n",
    "        with tf.name_scope(\"input_normalization\"):\n",
    "            last_layer = tf.layers.batch_normalization(last_layer)\n",
    "\n",
    "        # add convolutional layers\n",
    "        for idx, output_dim in enumerate(convolutional_dimensions):\n",
    "            with tf.name_scope(\"convolutions\"):\n",
    "                last_layer = self.get_conv_layer(last_layer, output_dim, [3, 3], \"SAME\", tf.nn.relu)\n",
    "            with tf.name_scope(\"max_pooling\"):\n",
    "                last_layer = tf.layers.max_pooling2d(last_layer, pool_size=[3, 3], strides=2, padding=\"SAME\")\n",
    "            #with tf.name_scope(\"normalization\"):\n",
    "                #last_layer = tf.layers.batch_normalization(last_layer)\n",
    "        \n",
    "        # flatten the last convolution\n",
    "        last_layer = tf.contrib.layers.flatten(last_layer)\n",
    "\n",
    "        # add dense layers\n",
    "        for idx, output_dim in enumerate(dense_layer_dimensions):\n",
    "            with tf.name_scope(\"dense\"):\n",
    "                last_layer = tf.layers.dense(inputs=last_layer, units=output_dim, activation=tf.nn.relu)\n",
    "\n",
    "        # last layer is always dimension (?, 10)\n",
    "        logits = tf.layers.dense(inputs=last_layer, units=10, name=\"logits\")\n",
    "        \n",
    "    def define_optimization(self, learning_rate=0.01):\n",
    "        default_graph = tf.get_default_graph()\n",
    "        y = default_graph.get_tensor_by_name(\"input/y_input:0\")\n",
    "        global_step = default_graph.get_tensor_by_name(\"input/global_step:0\")\n",
    "        logits = default_graph.get_tensor_by_name(\"logits/BiasAdd:0\")\n",
    "        \n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "                xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y, 10), logits=logits)\n",
    "                with tf.name_scope(\"loss\"):\n",
    "                    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        with tf.name_scope(\"softmax\"):\n",
    "            # Softmax layer\n",
    "            softmax = tf.nn.softmax(logits)\n",
    "            # Predict train\n",
    "            y_hat = tf.argmax(softmax, axis=1, name=\"y_hat\")\n",
    "            \n",
    "        with tf.name_scope('train'):\n",
    "            optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(xentropy, name=\"training_op\")\n",
    " \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # Train Evaluation\n",
    "            train_correct = tf.equal(y, y_hat)\n",
    "            tf.reduce_mean(tf.cast(train_correct, tf.float64), name=\"acc\")\n",
    "\n",
    "\n",
    "    def train(self, sess, train_x=None, train_y=None, n_epochs=1, batch_size=100):\n",
    "        \n",
    "        # New***\n",
    "        init = tf.global_variables_initializer() \n",
    "        \n",
    "        # so we can load the graph into memory and train with data that we input\n",
    "        # or data that we've saved\n",
    "        if train_x == None:\n",
    "            train_x = self.train_x\n",
    "        if train_y == None:\n",
    "            train_y = self.train_y\n",
    "\n",
    "        # get default graph\n",
    "        default_graph = tf.get_default_graph()\n",
    "        \n",
    "        # Get individual graph tensors\n",
    "        X = default_graph.get_tensor_by_name(\"input/x_input:0\")\n",
    "        Y = default_graph.get_tensor_by_name(\"input/y_input:0\")\n",
    "        global_step = default_graph.get_tensor_by_name(\"input/global_step:0\")\n",
    "        \n",
    "        # Get specific operations in graph\n",
    "        training_op = default_graph.get_operation_by_name(\"train/training_op\")\n",
    "        accuracy = default_graph.get_tensor_by_name(\"accuracy/acc:0\")\n",
    "        loss = default_graph.get_tensor_by_name(\"cross_entropy/loss/loss:0\")\n",
    "        \n",
    "        # Create logdir\n",
    "        now = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"tf_logs\"\n",
    "        logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "        logdir = f\"{root_logdir}/run\"\n",
    "        \n",
    "        # Create Tensorboard file_writers\n",
    "        summary_writer = tf.summary.FileWriter(logdir, default_graph)\n",
    "        \n",
    "        # Create Tensorboard Summaries \n",
    "        acc_train_summary = tf.summary.scalar(\"train_accuracy\", accuracy)\n",
    "        acc_test_summary = tf.summary.scalar(\"test_accuracy\", accuracy)\n",
    "        loss_summary = tf.summary.scalar('train_loss', loss)\n",
    "        \n",
    "        # Create Tensorboard histograms\n",
    "        conv_histograms = []\n",
    "        for i in tf.global_variables():\n",
    "            if i.name[:6] == \"conv2d\":\n",
    "                # need to switch colon with underscore because colons aren't allowed in summary names\n",
    "                conv_histograms.append(tf.summary.histogram(i.name.replace(\":\", \"_\"), i))\n",
    "        \n",
    "        # Wrapper tells Tensorboard to assume everything is within same session\n",
    "        sess.run(init)\n",
    "\n",
    "        step = 0\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            n_iters = int((self.num_examples * (1 - self.perc_test)) / batch_size)\n",
    "            print_interval = 100\n",
    "            best_test = 0.\n",
    "\n",
    "            print(f\"\\nStarting epoch {epoch}, running for {n_iters} iterations\")\n",
    "            print(f\"Printing evaluation metrics every {print_interval} iterations\\n\")\n",
    "\n",
    "            for iteration in range(1, n_iters):\n",
    "                step += 1\n",
    "                indices = np.random.choice(int(self.num_examples * (1 - self.perc_test)), batch_size)\n",
    "                X_batch = self.train_x[indices]\n",
    "                y_batch = self.train_y[indices]\n",
    "                sess.run(training_op, feed_dict={X: X_batch,\n",
    "                                                 Y: y_batch,\n",
    "                                                 global_step: step})\n",
    "\n",
    "                if iteration % print_interval == 0:\n",
    "\n",
    "                    acc_train, train_summary = sess.run([accuracy, tf.summary.merge([loss_summary, acc_train_summary])],\n",
    "                                                        {X: X_batch, Y: y_batch})\n",
    "\n",
    "                    acc_test, test_summary = sess.run([accuracy, acc_test_summary], {X: self.test_x, Y: self.test_y})\n",
    "\n",
    "                    # Add batch train loss & accuracy to Tensorboard\n",
    "                    merged_histograms = tf.summary.merge([i.eval(session=sess) for i in conv_histograms])\n",
    "                    summary_writer.add_summary(tf.summary.merge([train_summary, test_summary, merged_histograms]).eval(session=sess), step)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "                    # Print out batch evaluation metrics\n",
    "                    print(\"Iteration: {} train acc: {:.4f} test acc: {:.4f}\".format(iteration, \n",
    "                                                                                    acc_train, \n",
    "                                                                                    acc_test))\n",
    "        # Ensure file_writers closed\n",
    "        summary_writer.close() \n",
    "        \n",
    "    def get_conv_layer(self, input_layer, filter_num, ksize, padding, activation):\n",
    "\n",
    "        conv_layer = tf.layers.conv2d(inputs = input_layer,\n",
    "                                     filters = filter_num,\n",
    "                                     kernel_size = ksize,\n",
    "                                     padding = padding,\n",
    "                                     activation=activation)\n",
    "        return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 1, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.3900 test acc: 0.2822\n",
      "Iteration: 200 train acc: 0.5300 test acc: 0.4666\n",
      "Iteration: 300 train acc: 0.6600 test acc: 0.5406\n",
      "Iteration: 400 train acc: 0.7100 test acc: 0.5760\n",
      "\n",
      "Starting epoch 2, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.7100 test acc: 0.6296\n",
      "Iteration: 200 train acc: 0.7900 test acc: 0.6514\n",
      "Iteration: 300 train acc: 0.6800 test acc: 0.6598\n",
      "Iteration: 400 train acc: 0.7600 test acc: 0.6636\n",
      "\n",
      "Starting epoch 3, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.7900 test acc: 0.6774\n",
      "Iteration: 200 train acc: 0.8600 test acc: 0.6898\n",
      "Iteration: 300 train acc: 0.7500 test acc: 0.7002\n",
      "Iteration: 400 train acc: 0.8500 test acc: 0.6918\n",
      "\n",
      "Starting epoch 4, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.8500 test acc: 0.7122\n",
      "Iteration: 200 train acc: 0.8400 test acc: 0.7288\n",
      "Iteration: 300 train acc: 0.8400 test acc: 0.7342\n",
      "Iteration: 400 train acc: 0.8600 test acc: 0.7352\n",
      "\n",
      "Starting epoch 5, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.9100 test acc: 0.7278\n",
      "Iteration: 200 train acc: 0.9200 test acc: 0.7268\n",
      "Iteration: 300 train acc: 0.9400 test acc: 0.7454\n",
      "Iteration: 400 train acc: 0.9100 test acc: 0.7458\n",
      "\n",
      "Starting epoch 6, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.9200 test acc: 0.7476\n",
      "Iteration: 200 train acc: 0.9100 test acc: 0.7528\n",
      "Iteration: 300 train acc: 0.8600 test acc: 0.7430\n",
      "Iteration: 400 train acc: 0.9600 test acc: 0.7516\n",
      "\n",
      "Starting epoch 7, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.9500 test acc: 0.7562\n",
      "Iteration: 200 train acc: 0.9300 test acc: 0.7658\n",
      "Iteration: 300 train acc: 0.9800 test acc: 0.7566\n",
      "Iteration: 400 train acc: 1.0000 test acc: 0.7638\n",
      "\n",
      "Starting epoch 8, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.9700 test acc: 0.7660\n",
      "Iteration: 200 train acc: 0.9800 test acc: 0.7704\n",
      "Iteration: 300 train acc: 0.9900 test acc: 0.7710\n",
      "Iteration: 400 train acc: 0.9600 test acc: 0.7674\n",
      "\n",
      "Starting epoch 9, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.9900 test acc: 0.7778\n",
      "Iteration: 200 train acc: 0.9900 test acc: 0.7710\n",
      "Iteration: 300 train acc: 0.9800 test acc: 0.7620\n",
      "Iteration: 400 train acc: 0.9900 test acc: 0.7682\n",
      "\n",
      "Starting epoch 10, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 1.0000 test acc: 0.7692\n",
      "Iteration: 200 train acc: 0.9900 test acc: 0.7646\n",
      "Iteration: 300 train acc: 0.9700 test acc: 0.7710\n",
      "Iteration: 400 train acc: 0.9900 test acc: 0.7752\n",
      "\n",
      "Starting epoch 11, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 1.0000 test acc: 0.7692\n",
      "Iteration: 200 train acc: 0.9900 test acc: 0.7684\n",
      "Iteration: 300 train acc: 1.0000 test acc: 0.7754\n",
      "Iteration: 400 train acc: 1.0000 test acc: 0.7698\n",
      "\n",
      "Starting epoch 12, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.9900 test acc: 0.7730\n",
      "Iteration: 200 train acc: 1.0000 test acc: 0.7778\n",
      "Iteration: 300 train acc: 1.0000 test acc: 0.7754\n",
      "Iteration: 400 train acc: 1.0000 test acc: 0.7784\n",
      "\n",
      "Starting epoch 13, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 1.0000 test acc: 0.7848\n",
      "Iteration: 200 train acc: 1.0000 test acc: 0.7816\n",
      "Iteration: 300 train acc: 1.0000 test acc: 0.7784\n",
      "Iteration: 400 train acc: 1.0000 test acc: 0.7820\n",
      "\n",
      "Starting epoch 14, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 1.0000 test acc: 0.7654\n",
      "Iteration: 200 train acc: 1.0000 test acc: 0.7776\n",
      "Iteration: 300 train acc: 1.0000 test acc: 0.7790\n",
      "Iteration: 400 train acc: 1.0000 test acc: 0.7812\n",
      "\n",
      "Starting epoch 15, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 1.0000 test acc: 0.7760\n",
      "Iteration: 200 train acc: 1.0000 test acc: 0.7790\n",
      "Iteration: 300 train acc: 1.0000 test acc: 0.7742\n",
      "Iteration: 400 train acc: 1.0000 test acc: 0.7834\n",
      "\n",
      "Starting epoch 16, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.9900 test acc: 0.7790\n",
      "Iteration: 200 train acc: 1.0000 test acc: 0.7812\n",
      "Iteration: 300 train acc: 1.0000 test acc: 0.7822\n",
      "Iteration: 400 train acc: 1.0000 test acc: 0.7784\n",
      "\n",
      "Starting epoch 17, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 1.0000 test acc: 0.7786\n",
      "Iteration: 200 train acc: 1.0000 test acc: 0.7824\n",
      "Iteration: 300 train acc: 1.0000 test acc: 0.7776\n",
      "Iteration: 400 train acc: 1.0000 test acc: 0.7804\n",
      "\n",
      "Starting epoch 18, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 1.0000 test acc: 0.7778\n",
      "Iteration: 200 train acc: 1.0000 test acc: 0.7830\n",
      "Iteration: 300 train acc: 1.0000 test acc: 0.7818\n",
      "Iteration: 400 train acc: 1.0000 test acc: 0.7792\n",
      "\n",
      "Starting epoch 19, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 1.0000 test acc: 0.7818\n",
      "Iteration: 200 train acc: 1.0000 test acc: 0.7806\n",
      "Iteration: 300 train acc: 1.0000 test acc: 0.7788\n",
      "Iteration: 400 train acc: 1.0000 test acc: 0.7834\n",
      "\n",
      "Starting epoch 20, running for 450 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 1.0000 test acc: 0.7844\n",
      "Iteration: 200 train acc: 1.0000 test acc: 0.7806\n",
      "Iteration: 300 train acc: 1.0000 test acc: 0.7828\n",
      "Iteration: 400 train acc: 1.0000 test acc: 0.7836\n"
     ]
    }
   ],
   "source": [
    "cnn = CIFAR10_CNN(train_x, train_y)\n",
    "cnn.build_graph(convolutional_dimensions=[96,192], dense_layer_dimensions=[1024])\n",
    "cnn.define_optimization(learning_rate=0.01)\n",
    "sess = tf.Session()\n",
    "cnn.train(sess, train_x=None, train_y=None, n_epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "# After training, run this cell to generate the predictions file ans1-uni.npy\n",
    "predictions_filename = \"ans1-uni.npy\"\n",
    "g = tf.get_default_graph()\n",
    "logits = g.get_operation_by_name(\"logits/BiasAdd\").outputs[0]\n",
    "X = g.get_tensor_by_name(\"input/x_input:0\")\n",
    "\n",
    "# sess should still exist from training in the previous cell\n",
    "y_unnormalized_prediction = sess.run(logits, {X: test_x}).T\n",
    "with open(predictions_filename, \"wb+\") as f:\n",
    "    np.save(f, y_unnormalized_prediction)\n",
    "\n",
    "# confirm saved values are in the right dimension (10, N)\n",
    "with open(predictions_filename, \"rb\") as f:\n",
    "    v = np.load(f)\n",
    "    print(v.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
